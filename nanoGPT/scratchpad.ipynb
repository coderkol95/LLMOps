{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "encoder = tiktoken.get_encoding(\"gpt2\")\n",
    "device = torch.device(\"mps\")\n",
    "block_size = 32\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input.txt\",\"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "n = len(data)\n",
    "train_data = data[:int(0.9*n)]\n",
    "val_data = data[int(0.9*n):]\n",
    "\n",
    "# Encode with tiktoken gpt2 BPE\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "train_ids = enc.encode_ordinary(train_data)\n",
    "val_ids = enc.encode_ordinary(val_data)\n",
    "\n",
    "# Export to binaries\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile(\"train.bin\")\n",
    "val_ids.tofile(\"val.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better understanding of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "\n",
    "    if split == \"train\":\n",
    "        data = np.memmap(\"train.bin\", dtype=np.uint16, mode=\"r\")\n",
    "    else:\n",
    "        data = np.memmap(\"val.bin\", dtype=np.uint16, mode=\"r\")\n",
    "\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([ 5962, 22307,    25,   198,  8421,   356,  5120,   597,  2252,\n",
       "           11,  3285,   502,  2740,    13,   198,   198,  3237,    25,\n",
       "          198,  5248,   461,    11,  2740,    13,   198,   198,  5962,\n",
       "        22307,    25,   198,  1639,   389,   477, 12939,  2138,   284,\n",
       "         4656,   621,   284,  1145,   680,    30,   198,   198,  3237,\n",
       "           25,   198,  4965,  5634,    13, 12939,    13,   198,   198,\n",
       "         5962, 22307,    25,   198,  5962,    11,   345,   760,   327,\n",
       "         1872,   385,  1526, 28599,   318,  4039,  4472,   284,   262,\n",
       "          661,    13,   198,   198,  3237,    25,   198,  1135,   760,\n",
       "          470,    11,   356,   760,   470,    13,   198,   198,  5962,\n",
       "        22307,    25,   198,  5756,   514,  1494,   683,    11,   290,\n",
       "          356,  1183,   423, 11676,   379,   674,   898,  2756,    13,\n",
       "          198,  3792,   470,   257, 15593,    30,   198,   198,  3237,\n",
       "           25,   198,  2949,   517,  3375,   319,   470,    26,  1309,\n",
       "          340,   307,  1760,    25], dtype=uint16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.memmap(\"train.bin\", dtype=np.uint16, mode=\"r\")\n",
    "data[:130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([173880,  79329, 300612, 263019, 209280, 216553, 247104,   5622])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix = torch.randint(len(data)-block_size, (batch_size,))\n",
    "ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(301966, 32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data), block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, y_tr=get_batch(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 32]), torch.Size([8, 32]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr.shape, y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4467,   286, 11906,  1751,  1364, 17903,    25,   198,  1537,  1918,\n",
       "         22027, 43388,  1549,   616,  5229,   422,  6164,  5101,    11,   198,\n",
       "          1870,   458,  1347,  1549,   734,  1067,   315,  2052,   422,   616,\n",
       "         46299, 21755],\n",
       "        [  373,   339,    13,   198,   198,  8763,  2606,    34,  1546,  5781,\n",
       "            25,   198,   464,  2116, 31642,  1438,    11,   475,   530,   286,\n",
       "          1365,  3450,    13,   198,   198,    43,  2885,    56,  3537, 12161,\n",
       "            25,   198],\n",
       "        [  198,  1639,   550,   881, 50129,   284,   787,   465, 18021,  1745,\n",
       "            25,   198,  2215,   345,  3350,   503,    11,   340,   991,  1625,\n",
       "          1363,    13,   198,   198,  2538, 35830,  1546,    25,   198, 11633,\n",
       "           301,  3465]], device='mps:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing data through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of paramaters:  3.31808\n"
     ]
    }
   ],
   "source": [
    "from model import GPTConfig, GPT\n",
    "config = GPTConfig()\n",
    "model = GPT(config=config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.3952e-02, -3.3385e-04,  1.7458e-02,  ...,  3.2066e-01,\n",
       "           1.4161e-01,  1.8533e-01],\n",
       "         [-3.2040e-02,  8.3955e-02, -1.1653e-01,  ..., -1.4415e-01,\n",
       "          -1.0661e-01,  1.3936e-01],\n",
       "         [-4.5587e-03,  2.4526e-01, -1.3623e-01,  ..., -4.5750e-04,\n",
       "          -7.6701e-02,  3.0728e-01],\n",
       "         ...,\n",
       "         [-7.0332e-02, -1.2510e-01, -4.0615e-01,  ...,  3.6386e-01,\n",
       "           1.0711e-01,  5.3783e-01],\n",
       "         [-3.2356e-01,  1.3388e-01,  2.3756e-01,  ..., -1.5389e-01,\n",
       "          -3.9181e-03, -2.8393e-02],\n",
       "         [ 2.1391e-01,  2.0663e-02, -2.5889e-01,  ..., -1.9707e-01,\n",
       "           1.1265e-01,  3.3093e-01]],\n",
       "\n",
       "        [[ 1.9286e-01,  5.3123e-02, -6.7029e-02,  ..., -1.9901e-02,\n",
       "           1.4604e-01, -5.5481e-02],\n",
       "         [-3.5802e-01, -1.9375e-01,  9.9082e-02,  ...,  9.4947e-02,\n",
       "          -2.1083e-02, -2.0803e-01],\n",
       "         [ 1.3091e-01,  2.4485e-01, -4.4572e-02,  ..., -1.8932e-01,\n",
       "           1.0870e-01, -1.0794e-03],\n",
       "         ...,\n",
       "         [-2.0336e-01,  3.9552e-02,  5.7850e-02,  ...,  6.1716e-02,\n",
       "           1.3437e-01,  2.5733e-01],\n",
       "         [-3.4349e-01,  7.3139e-02,  4.5315e-02,  ...,  1.8335e-01,\n",
       "           1.1688e-01,  2.4940e-01],\n",
       "         [ 2.6567e-01,  5.8921e-02, -1.8515e-01,  ..., -8.0870e-02,\n",
       "          -8.7417e-02,  7.8298e-02]],\n",
       "\n",
       "        [[-4.3312e-02,  6.0338e-03,  8.4258e-02,  ...,  1.4935e-01,\n",
       "           5.0407e-02, -1.0340e-01],\n",
       "         [-1.7447e-01,  1.3369e-01,  1.9150e-03,  ..., -9.1264e-02,\n",
       "           2.4030e-02, -1.1505e-02],\n",
       "         [ 4.8142e-02,  2.9463e-01, -3.5868e-02,  ...,  1.7353e-02,\n",
       "          -4.9787e-02,  6.1997e-02],\n",
       "         ...,\n",
       "         [-1.7929e-01, -9.5240e-02, -4.9616e-02,  ...,  3.5581e-01,\n",
       "           1.0096e-01,  3.8879e-02],\n",
       "         [-2.3805e-01,  1.5713e-01,  1.8985e-01,  ..., -1.1223e-01,\n",
       "          -1.9507e-01,  1.0782e-01],\n",
       "         [ 3.2951e-01,  1.5778e-01, -2.0336e-01,  ..., -2.5089e-02,\n",
       "          -8.8860e-02,  3.4736e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.7569e-01, -1.8115e-01, -1.2247e-01,  ...,  2.8019e-01,\n",
       "           2.5346e-01, -2.7740e-02],\n",
       "         [-3.4677e-01, -2.0795e-01,  4.2856e-03,  ..., -6.9606e-02,\n",
       "          -1.8734e-01, -3.8619e-02],\n",
       "         [ 5.5468e-04,  2.0489e-01, -1.8493e-01,  ..., -2.4975e-01,\n",
       "           1.9487e-01,  1.6934e-01],\n",
       "         ...,\n",
       "         [-2.4909e-01, -8.2261e-02,  1.2921e-01,  ...,  8.1989e-02,\n",
       "           1.9525e-01, -1.1699e-02],\n",
       "         [-1.7951e-01,  1.7764e-01,  1.7259e-01,  ..., -6.3666e-02,\n",
       "          -8.8896e-02,  1.5403e-01],\n",
       "         [ 1.2682e-01, -8.2599e-02, -1.2175e-02,  ..., -5.3808e-02,\n",
       "           2.2151e-01, -7.1889e-02]],\n",
       "\n",
       "        [[ 1.0956e-01,  2.1262e-02, -5.5557e-02,  ...,  2.9283e-01,\n",
       "          -1.0881e-03,  2.5900e-02],\n",
       "         [-1.4937e-01, -7.7329e-02, -5.3066e-02,  ...,  4.3176e-02,\n",
       "          -3.8562e-02, -6.6031e-02],\n",
       "         [ 7.3574e-02,  6.8963e-02, -8.0557e-02,  ..., -7.5477e-02,\n",
       "           6.7098e-02,  9.2039e-02],\n",
       "         ...,\n",
       "         [-2.3584e-01, -2.5530e-01, -1.0935e-01,  ...,  1.3204e-01,\n",
       "           2.4401e-01,  2.1571e-01],\n",
       "         [-9.6005e-02,  1.3301e-01,  1.3119e-02,  ..., -1.2539e-02,\n",
       "          -6.8610e-02,  3.2543e-01],\n",
       "         [ 1.2697e-01, -1.7009e-01, -4.0767e-01,  ..., -2.3964e-01,\n",
       "           1.1016e-01, -1.9891e-02]],\n",
       "\n",
       "        [[-1.1124e-01, -3.1737e-03, -4.0123e-03,  ...,  2.8116e-01,\n",
       "           1.6250e-01, -1.2901e-02],\n",
       "         [-7.2535e-02,  1.2467e-01,  1.3077e-01,  ..., -7.8692e-03,\n",
       "          -6.6035e-02, -1.4550e-01],\n",
       "         [-5.8697e-02,  3.0670e-01, -5.9917e-02,  ...,  1.4969e-01,\n",
       "           1.9498e-02,  1.3426e-01],\n",
       "         ...,\n",
       "         [-1.4270e-02,  4.5309e-02, -2.4330e-01,  ...,  1.8797e-01,\n",
       "           7.7275e-02,  3.0316e-01],\n",
       "         [-1.5090e-01,  1.6722e-01,  3.5521e-01,  ..., -1.5000e-01,\n",
       "           1.7010e-01, -6.7756e-02],\n",
       "         [ 2.5598e-01,  1.0290e-01,  1.2033e-01,  ..., -1.2209e-01,\n",
       "          -3.4772e-02, -3.6754e-02]]], device='mps:0',\n",
       "       grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, loss = model(x_tr, y_tr)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32, 50304])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 6e-4\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 10, with 3,319,808 parameters\n",
      "num non-decayed parameter tensors: 5, with 320 parameters\n"
     ]
    }
   ],
   "source": [
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((301966,), torch.Size([8, 32]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, x_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4467,   286, 11906,  1751,  1364, 17903,    25,   198,  1537,  1918,\n",
       "        22027, 43388,  1549,   616,  5229,   422,  6164,  5101,    11,   198,\n",
       "         1870,   458,  1347,  1549,   734,  1067,   315,  2052,   422,   616,\n",
       "        46299, 21755], device='mps:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Loss: 10.7726\n",
      "Epoch: [2/10], Loss: 10.6957\n",
      "Epoch: [3/10], Loss: 10.591\n",
      "Epoch: [4/10], Loss: 10.51\n",
      "Epoch: [5/10], Loss: 10.4492\n",
      "Epoch: [6/10], Loss: 10.3302\n",
      "Epoch: [7/10], Loss: 10.1916\n",
      "Epoch: [8/10], Loss: 10.1491\n",
      "Epoch: [9/10], Loss: 10.0197\n",
      "Epoch: [10/10], Loss: 9.986\n"
     ]
    }
   ],
   "source": [
    "loss_dict = {}\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for eval_iters in np.arange(2):\n",
    "        x_tr, y_tr = get_batch(\"train\")\n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(x_tr, y_tr)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch: [{epoch+1}/{num_epochs}], Loss: {np.round(loss.item(),4)}\")\n",
    "    loss_dict.update({epoch:loss.item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
